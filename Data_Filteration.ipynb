{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BM_PI(data, jname):\n",
    "    NullLinks = []\n",
    "    links = []\n",
    "    ll = []\n",
    "    lst = []\n",
    "    start_time = time.time()\n",
    "    listoflist = []\n",
    "    d=[]\n",
    "    implink = []\n",
    "    notimp = []\n",
    "    somelist = []\n",
    "    Arraypz = []\n",
    "\n",
    "    for i, item in enumerate(data['pages']):\n",
    "        wiki = item['wikifier']\n",
    "            \n",
    "        page = data['pages'][i]['pid']\n",
    "        countz = collections.Counter(page)\n",
    "        if wiki == []:\n",
    "                continue\n",
    "        else:\n",
    "            for j, field in enumerate(data['pages'][i]['wikifier']):\n",
    "                k = field['link']\n",
    "                lst=[]\n",
    "                lst.append(k)\n",
    "                if k == None:\n",
    "                    \n",
    "                    links.append(\"Chicago School\")\n",
    "                    lst.remove(None)\n",
    "                    #links.remove(None)\n",
    "                if len(lst) == 0:\n",
    "                    lst.append(\"Chicago School\")\n",
    "                else:                        \n",
    "                    links.append(k)\n",
    "                       \n",
    "    unique_list = [\n",
    "    e\n",
    "    for i, e in enumerate(links)\n",
    "    if links.index(e) == i\n",
    "    ]\n",
    "    counter=0   \n",
    "\n",
    "    pages = data[\"pages\"]\n",
    "\n",
    "    ''' for j, page in enumerate(pages):\n",
    "        wikiR = page[\"wikifier\"]\n",
    "        lst = []\n",
    "        for link in wikiR:\n",
    "        \n",
    "            lst.append(link['link']) \n",
    "            if link['link'] == None:\n",
    "                lst.remove(None)\n",
    "        \n",
    "        if len(lst) == 0:\n",
    "            lst.append(\"Chicago School\")'''\n",
    "    \n",
    "    ll.append(lst)\n",
    "\n",
    "    arraymat = np.array(ll)\n",
    "\n",
    "    N = len(unique_list)\n",
    "\n",
    "    M = np.zeros((N, N), dtype=np.int32)\n",
    "\n",
    "    for l,dont in enumerate(arraymat):\n",
    "  \n",
    "        if dont == None:\n",
    "            dont.remove(None)   \n",
    "         \n",
    "        for fc in range(0,(len(dont)-1)):\n",
    "               \n",
    "            k = unique_list.index(dont[fc])\n",
    "            m = unique_list.index(dont[fc+1])\n",
    "            M[k][m]+=1\n",
    "    \n",
    "            for tw in range(fc,(len(dont)-2)):\n",
    "                k = unique_list.index(dont[tw])\n",
    "                m = unique_list.index(dont[tw+2])\n",
    "\n",
    "                M[k][m]+=1\n",
    "        d.append(dont[0])\n",
    "        d.sort()\n",
    "        count = collections.Counter(d) \n",
    "    M = normalize(M, norm=\"l1\", axis=0)\n",
    "    Arraypz=[]\n",
    "    Arraypz = np.zeros((N,1), dtype=\"int32\")\n",
    "\n",
    "    for il,ll in enumerate(unique_list):\n",
    "\n",
    "        if ll in d:\n",
    "            pk = unique_list.index(ll)            \n",
    "            Arraypz[pk]=count.get(ll)\n",
    "        else:\n",
    "            pt = unique_list.index(ll)\n",
    "            count[ll]=0\n",
    "            Arraypz[pt] = 0\n",
    "    \n",
    "    FM = np.dot(M,Arraypz)\n",
    "    SM=FM*0.8\n",
    "    Final = SM + (Arraypz*0.2)\n",
    "\n",
    "    \n",
    "    FinalMatrix = (((np.dot(M,Final)*0.8)+(Arraypz*0.2)))\n",
    "    percentile = np.percentile(FinalMatrix, 80)\n",
    "\n",
    "    for i in range(0, len(FinalMatrix)-1):\n",
    "        if FinalMatrix[i][0] > percentile:\n",
    "            somelist.append(i)\n",
    "        \n",
    "    for j, k in enumerate(somelist):\n",
    "        implink.append(unique_list[k])\n",
    "        \n",
    "    for ls, lsk in enumerate(unique_list):\n",
    "        if lsk not in implink:\n",
    "            notimp.append(lsk)\n",
    "    \n",
    "    book2 = open(jsonfile, encoding='utf-8')\n",
    "    with book2 as data_file:\n",
    "        dataaa = json.load(data_file)\n",
    "        \n",
    "    for i, item in enumerate(dataaa['pages']):\n",
    "        for j, field in enumerate(dataaa['pages'][i]['wikifier']):\n",
    "        \n",
    "            kooo=field['link']\n",
    "            if kooo in notimp:\n",
    "                del dataaa['pages'][i]['wikifier'][j]\n",
    "            #dataaa['pages'][i]['wikifier'][j]['link'] = None\n",
    "    upd = os.path.join(\"UpdatedData/\", jname)\n",
    "    with open(upd, 'w') as data_file:\n",
    "        dataaa = json.dump(dataaa, data_file)\n",
    "    \n",
    "    #jsonupdate(jsonfile)\n",
    "    \n",
    "    \n",
    "    \n",
    "           \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.tar\\metadata\\meta\\mdp.39015062797538.json\n",
      "JSON Books\\mdp.39015062797538.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rima\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:67: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "C:\\Users\\Rima\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.tar\\metadata\\meta\\mdp.39015064582888.json\n",
      "JSON Books\\mdp.39015064582888.json\n",
      "metadata.tar\\metadata\\meta\\nnc1.ar53666712.json\n",
      "JSON Books\\nnc1.ar53666712.json\n",
      "metadata.tar\\metadata\\meta\\uc1.$b715276.json\n",
      "JSON Books\\uc1.$b715276.json\n",
      "metadata.tar\\metadata\\meta\\uc1.b3819355.json\n",
      "metadata.tar\\metadata\\meta\\uiug.30112078740336.json\n",
      "metadata.tar\\metadata\\meta\\uva.x001197927.json\n",
      "JSON Books\\uva.x001197927.json\n",
      "6.500078201293945\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import collections, numpy\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "NullLinks=[]\n",
    "metapath = \"metadata.tar\\metadata\\meta\"\n",
    "jsonpath = \"JSON Books\"\n",
    "\n",
    "stime = time.time()\n",
    "for k in os.listdir(jsonpath):\n",
    "    #if k == '.DS_Store': continue\n",
    "    jname = k\n",
    "    jsonfile = os.path.join(jsonpath, k)\n",
    "    with open(jsonfile, encoding=\"utf-8\") as data_jfile:\n",
    "        data = json.load(data_jfile)\n",
    "    \n",
    "    metafile = os.path.join(metapath, k)\n",
    "    print(metafile)\n",
    "    if os.path.exists(metafile):\n",
    "        \n",
    "        with open(metafile, encoding=\"utf-8\") as data_mfile:   \n",
    "                metadata = json.load(data_mfile)\n",
    "\n",
    "                if metadata['bibliographicFormat'] == \"BK\":\n",
    "                    print(jsonfile)\n",
    "                    BM_PI(data, jname)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "                else:\n",
    "                    jsonfile = os.path.join(jsonpath, k)\n",
    "                    if os.path.exists(jsonfile):\n",
    "                        with open(jsonfile, encoding=\"utf-8\") as data_jfile:   \n",
    "                            jsondata = json.load(data_jfile)\n",
    "            \n",
    "                        for i, item in enumerate(jsondata['pages']):\n",
    "                            wiki = item['wikifier']\n",
    "                        \n",
    "                            if wiki == []:\n",
    "                                continue\n",
    "                            else:\n",
    "                                for j, field in enumerate(jsondata['pages'][i]['wikifier']):\n",
    "                                    k = field['link']\n",
    "\n",
    "                                    if k == None:\n",
    "                                        for j, field in enumerate(jsondata['pages'][i]['wikifier']):\n",
    "                                                l = field['link']\n",
    "                                                NullLinks.append(l)\n",
    "                                               \n",
    "                                    else:                        \n",
    "                                        continue\n",
    "          \n",
    "                \n",
    "etime = time.time()   \n",
    "print(etime-stime)\n",
    "#print(NullLinks)\n",
    "      \n",
    "                    \n",
    "           \n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
